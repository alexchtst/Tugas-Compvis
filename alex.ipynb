{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4S8CzjAhN_"
      },
      "source": [
        "## MODEL PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8CN5niC_X7S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"./data/gray_data.csv\"\n",
        "\n",
        "gray_image_path = \"./\"\n",
        "\n",
        "# gray_image_path = \"/content/drive/MyDrive/datacompvis/grayimage/\"\n",
        "# file_path = \"/content/drive/MyDrive/datacompvis/gray_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "def flip(image, orientation=1):\n",
        "    if image is None:\n",
        "        raise ValueError(\"Image must not be None\")\n",
        "    return cv.flip(image, orientation)\n",
        "\n",
        "def rotate(image, angle=15):\n",
        "    (h, w) = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv.getRotationMatrix2D(center, angle, 1.0)\n",
        "    return cv.warpAffine(image, M, (w, h))\n",
        "\n",
        "def add_noise(image):\n",
        "    noise = np.random.normal(0, 10, image.shape).astype(np.uint8)\n",
        "    return cv.add(image, noise)\n",
        "\n",
        "def brighten(image, value=30):\n",
        "    return cv.convertScaleAbs(image, alpha=1, beta=value)\n",
        "\n",
        "def resize(np_img, w = 512, h = 512):\n",
        "    return cv.resize(np_img, dsize=(w, h), interpolation=cv.INTER_LINEAR)\n",
        "\n",
        "def Proccess(df = df, idx = None, biar_rapih = False):\n",
        "    if idx == None:\n",
        "        idx = random.randint(0, len(df['file_path'].tolist()))\n",
        "\n",
        "    sample_image_name = df['file_path'].tolist()[idx]\n",
        "    sample_image_path = os.path.join(gray_image_path, sample_image_name)\n",
        "    score = df['score'].tolist()[idx]\n",
        "\n",
        "    img_np = np.array(Image.open(sample_image_path))\n",
        "\n",
        "    if not biar_rapih:\n",
        "        # gausah didebug ini [biar apa biarin | pusing gw njir ngeliatnya]\n",
        "        print(f\"image idx: {idx} | score: {score} | filename: {sample_image_name}\")\n",
        "        print(f\"image size: {img_np.shape}\")\n",
        "\n",
        "    blank = np.zeros(shape=img_np.shape, dtype='uint8')\n",
        "\n",
        "    gaussBlur = cv.GaussianBlur(img_np, (175, 175), 0.3)\n",
        "    canny = cv.Canny(gaussBlur, 100, 100)\n",
        "\n",
        "    _, tresh = cv.threshold(img_np, 125, 255, cv.THRESH_BINARY)\n",
        "    _, blurtresh = cv.threshold(gaussBlur, 125, 255, cv.THRESH_BINARY)\n",
        "\n",
        "    contours, _ = cv.findContours(canny, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
        "    cv.drawContours(blank, contours, -1, 255, 1)\n",
        "\n",
        "    images = [img_np, gaussBlur, canny, tresh, blank]\n",
        "    titles = ['Gray', 'Gaussian Blur', 'Canny', 'Threshold', 'Contours Drawn']\n",
        "\n",
        "    return images, titles\n",
        "\n",
        "def ShowImage(images, titles):\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        plt.subplot(2, 3, i + 1)  # 2 baris, 3 kolom\n",
        "        if len(images[i].shape) == 2:  # grayscale\n",
        "            plt.imshow(images[i], cmap='gray')\n",
        "        else:  # RGB image\n",
        "            plt.imshow(cv.cvtColor(images[i], cv.COLOR_BGR2RGB))\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Before Augmenting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Jumlah data\n",
        "total_samples = len(df)\n",
        "train_size = int(0.75 * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "\n",
        "# Random split DataFrame\n",
        "train_df, val_df = random_split(df, [train_size, val_size])\n",
        "\n",
        "# Ambil indeks dari Subset\n",
        "train_indices = train_df.indices if hasattr(train_df, 'indices') else train_df\n",
        "val_indices = val_df.indices if hasattr(val_df, 'indices') else val_df\n",
        "\n",
        "# Buat list file path dan label dari df\n",
        "file_paths = df['file_path'].tolist()\n",
        "scores = df['score'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inisialisasi array kosong\n",
        "gray_np = []\n",
        "y_actual = []\n",
        "\n",
        "# Fungsi augmentasi sederhana\n",
        "def augment_pipeline(image):\n",
        "    aug_images = [\n",
        "        image,\n",
        "        flip(image),\n",
        "        flip(image, -1),\n",
        "        add_noise(image),\n",
        "        rotate(image, angle=15),\n",
        "        rotate(image, angle=30),\n",
        "        rotate(image, angle=60),\n",
        "    ]\n",
        "    return aug_images\n",
        "\n",
        "# LOOP: TRAIN SPLIT DULU\n",
        "for i in train_indices:\n",
        "    image_path = os.path.join(gray_image_path, file_paths[i])\n",
        "    image = resize(np.array(Image.open(image_path)))\n",
        "    print(image.shape)\n",
        "    label = scores[i]\n",
        "\n",
        "    for aug_img in augment_pipeline(image):\n",
        "        gray_np.append(aug_img)\n",
        "        y_actual.append(label)\n",
        "\n",
        "# VAL SET: tanpa augmentasi\n",
        "val_np = []\n",
        "val_labels = []\n",
        "\n",
        "for i in val_indices:\n",
        "    image_path = os.path.join(gray_image_path, file_paths[i])\n",
        "    image = resize(np.array(Image.open(image_path)))\n",
        "    print(image.shape)\n",
        "    val_np.append(image)\n",
        "    val_labels.append(scores[i])\n",
        "\n",
        "# Convert to NumPy\n",
        "gray_np = np.array(gray_np)\n",
        "val_np = np.array(val_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def show_augmented_images_3x3(image):\n",
        "    # Konversi ke grayscale\n",
        "    grayscale_image = Image.fromarray(image).convert('L')\n",
        "    gray_np = np.array(grayscale_image)\n",
        "\n",
        "    # Daftar augmentasi (tanpa brighten)\n",
        "    aug_images = [\n",
        "        gray_np,                    # original grayscale\n",
        "        flip(gray_np),\n",
        "        flip(gray_np, -1),\n",
        "        add_noise(gray_np),\n",
        "        rotate(gray_np, 15),\n",
        "        rotate(gray_np, 30),\n",
        "        rotate(gray_np, 60),                   # duplikat untuk mengisi grid ke-9\n",
        "    ]\n",
        "\n",
        "    # Tampilkan dalam grid 3x3\n",
        "    fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    for i, ax in enumerate(axs.flat):\n",
        "        if i < len(aug_images):  # Hanya tampilkan gambar sampai 7\n",
        "            ax.imshow(aug_images[i], cmap='gray')\n",
        "            # ax.set_title(f\"Augment {i+1}\")\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ambil satu gambar sebagai contoh\n",
        "image_path = 'data\\datasets\\\\5.png'\n",
        "image = resize(np.array(Image.open(image_path)))  # Pastikan fungsi resize sudah ada\n",
        "show_augmented_images_3x3(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gray_np.shape)\n",
        "print(len(y_actual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(val_np.shape)\n",
        "print(len(val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfzWmIX_AHt6",
        "outputId": "9a1d9931-88af-4ad9-df99-7ed34393c028"
      },
      "outputs": [],
      "source": [
        "max_w = max(df['width'].tolist())\n",
        "min_w = min(df['width'].tolist())\n",
        "\n",
        "max_h = max(df['height'].tolist())\n",
        "min_h = min(df['height'].tolist())\n",
        "\n",
        "print(f\"max-width: {max_w} | min-width: {min_w} | max-height: {max_h} | min-height: {min_h}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFp5gP8wAPNS"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    images, titles = Proccess()\n",
        "    ShowImage(images, titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(gray_np), len(y_actual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHPmLHKSQYJz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CannyContourDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx].astype(np.float32) / 255.0  # normalize ke [0, 1]\n",
        "        image = np.expand_dims(image, axis=0)  # tambahkan channel dimensi [1, H, W]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxv6iXrl2Gqo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_dataset = CannyContourDataset(gray_np, y_actual)\n",
        "val_dataset = CannyContourDataset(val_np, val_labels)\n",
        "# train_size = int(0.75 * len(dataset))\n",
        "# val_size = len(dataset) - train_size\n",
        "\n",
        "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "print(len(train_dataset), train_size, val_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opz0Bk2R2Jvt"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 64 * 512, 512)\n",
        "        # self.fc1 = nn.Linear(32 * 128 * 128, 128)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 512)\n",
        "        self.fc4 = nn.Linear(512, 526)\n",
        "        self.fc5 = nn.Linear(526, 128)\n",
        "        self.fc6 = nn.Linear(128, 64)\n",
        "        self.fc7 = nn.Linear(64, 32)\n",
        "        self.fc8 = nn.Linear(32, num_classes)\n",
        "\n",
        "        # Tambahkan dropout dengan probabilitas 0.5 (bisa disesuaikan)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # (16, 256, 128)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # (32, 128, 64)\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # (64, 64, 32)\n",
        "        x = x.view(x.size(0), -1)             # flatten to (B, 131072)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = self.fc8(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofjKY5W658-n",
        "outputId": "09464e37-da43-4109-bc73-d0f166aa1375"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "# Hitung jumlah tiap kelas\n",
        "label_counts = Counter(y_actual)\n",
        "total_samples = sum(label_counts.values())\n",
        "\n",
        "# Hitung bobot kelas: total / (jumlah kelas * count per class)\n",
        "num_classes = 5\n",
        "class_weights = []\n",
        "\n",
        "for i in range(num_classes):\n",
        "    count = label_counts.get(i, 1)  # supaya tidak divide by zero\n",
        "    weight = total_samples / (num_classes * count)\n",
        "    class_weights.append(weight)\n",
        "\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60y2lFWPUADV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(524288, 512)\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc5 = nn.Linear(128, 64)\n",
        "        self.bn7 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "        self.bn8 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.fc7 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (16, H/2, W/2)\n",
        "        x = self.pool(F.sigmoid(self.bn2(self.conv2(x))))  # (32, H/4, W/4)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.bn3(self.fc1(x)))\n",
        "        x = F.relu(self.bn4(self.fc2(x)))\n",
        "        x = F.relu(self.bn5(self.fc3(x)))\n",
        "        x = F.relu(self.bn6(self.fc4(x)))\n",
        "        x = F.relu(self.bn7(self.fc5(x)))\n",
        "        x = F.relu(self.bn8(self.fc6(x)))\n",
        "\n",
        "        x = self.fc7(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fz6I4632MzG",
        "outputId": "fd7a5614-5f5b-417b-f5fb-974fb35b4765"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gDvuYJw93zcw",
        "outputId": "6ad2886c-4cb9-423c-8797-102ed2fd9d54"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Hyperparameter Early Stopping\n",
        "patience = 10\n",
        "best_val_acc = 0.0\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(100):  # ubah jumlah epoch sesuai kebutuhan\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # VALIDASI\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0  # FIXED: Reset setiap epoch\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    scheduler.step(val_loss)  # <-- di sini tempatnya\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # EARLY STOPPING\n",
        "    if epoch > 10:\n",
        "      if val_acc > best_val_acc:\n",
        "          best_val_acc = val_acc\n",
        "          counter = 0\n",
        "          torch.save(model.state_dict(), 'best_model_hafidh_ver.pt')\n",
        "          print(\"Validation accuracy improved. Model saved.\")\n",
        "      else:\n",
        "          counter += 1\n",
        "          print(f\"No improvement. EarlyStopping counter: {counter}/{patience}\")\n",
        "          if counter >= patience:\n",
        "              print(\"Early stopping triggered.\")\n",
        "              break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz6OLLrRWe8r"
      },
      "outputs": [],
      "source": [
        "loaded_model = torch.load('best_model_hafidh_ver.pt')\n",
        "model.load_state_dict(loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgPif_ZEIpaI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "class_names = ['0', '1', '2', '3' ,'4']\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "print(f\"Akurasi Validasi: {100 * correct / total:.2f}%\")\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(cm.shape[1]), yticklabels=range(cm.shape[0]))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show\n",
        "\n",
        "#classification report\n",
        "print(\"\\nClassification Report :\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
